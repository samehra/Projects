{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWGzucuFfbBn"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/generation/langchain/handbook/00-langchain-intro.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/generation/langchain/handbook/00-langchain-intro.ipynb)\n",
        "\n",
        "#### [LangChain Handbook](https://github.com/pinecone-io/examples/tree/master/generation/langchain/handbook)\n",
        "\n",
        "# Intro to LangChain\n",
        "\n",
        "LangChain is a popular framework that allow users to quickly build apps and pipelines around **L**arge **L**anguage **M**odels. It can be used to for chatbots, **G**enerative **Q**uestion-**A**nwering (GQA), summarization, and much more.\n",
        "\n",
        "The core idea of the library is that we can _\"chain\"_ together different components to create more advanced use-cases around LLMs. Chains may consist of multiple components from several modules:\n",
        "\n",
        "* **Prompt templates**: Prompt templates are, well, templates for different types of prompts. Like \"chatbot\" style templates, ELI5 question-answering, etc\n",
        "\n",
        "* **LLMs**: Large language models like GPT-3, BLOOM, etc\n",
        "\n",
        "* **Agents**: Agents use LLMs to decide what actions should be taken, tools like web search or calculators can be used, and all packaged into logical loop of operations.\n",
        "\n",
        "* **Memory**: Short-term memory, long-term memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r-ryCeG_f_GC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7179e7e6-dd41-453a-c6d8-28aa76bb481d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.6/360.6 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 KB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNaXrEPOhbuL"
      },
      "source": [
        "# Using LLMs in LangChain\n",
        "\n",
        "LangChain supports several LLM providers, like Hugging Face and OpenAI.\n",
        "\n",
        "Let's start our exploration of LangChain by learning how to use a few of these different LLM integrations.\n",
        "\n",
        "## Hugging Face\n",
        "\n",
        "We first need to install additional prerequisite libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWA15ZkVjg80",
        "outputId": "66189430-1efc-4dc7-cfd0-8254ee5238db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/199.1 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m194.6/199.1 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-whfR5Tjf1O"
      },
      "source": [
        "For Hugging Face models we need a Hugging Face Hub API token. We can find this by first getting an account at [HuggingFace.co](https://huggingface.co/) and clicking on our profile in the top-right corner > click *Settings* > click *Access Tokens* > click *New Token* > set *Role* to *write* > *Generate* > copy and paste the token below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sRGTytxCjKaW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "HF_API_KEY = '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'\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = HF_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exAl3iQgnAra"
      },
      "source": [
        "We can then generate text using a HF Hub model (we'll use `google/flan-t5-x1`) using the Inference API built into Hugging Face Hub.\n",
        "\n",
        "_(The default Inference API doesn't use specialized hardware and so can be slow and cannot run larger models like `bigscience/bloom-560m` or `google/flan-t5-xxl`)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7yubiSJhIfs",
        "outputId": "fc57aa31-cf48-4fdb-d746-9fdb4b890163"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "green bay packers\n"
          ]
        }
      ],
      "source": [
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "\n",
        "# initialize HF LLM\n",
        "flan_t5 = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-t5-xl\",\n",
        "    model_kwargs={\"temperature\":1e-10}\n",
        ")\n",
        "\n",
        "# build prompt template for simple question-answering\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=flan_t5\n",
        ")\n",
        "\n",
        "question = \"Which NFL team won the Super Bowl in the 2010 season?\"\n",
        "\n",
        "print(llm_chain.run(question))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXroZSjCKxa2"
      },
      "source": [
        "If we'd like to ask multiple questions we can by passing a list of dictionary objects, where the dictionaries must contain the input variable set in our prompt template (`\"question\"`) that is mapped to the question we'd like to ask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jNZgxSIJsXj",
        "outputId": "eaa7eec6-3925-4dee-9a25-75014182f6d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='green bay packers', generation_info=None)], [Generation(text='184', generation_info=None)], [Generation(text='john glenn', generation_info=None)], [Generation(text='one', generation_info=None)]], llm_output=None)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "qs = [\n",
        "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
        "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
        "    {'question': \"Who was the 12th person on the moon?\"},\n",
        "    {'question': \"How many eyes does a blade of grass have?\"}\n",
        "]\n",
        "res = llm_chain.generate(qs)\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zoxlXHYLQix"
      },
      "source": [
        "It is a LLM, so we can try feeding in all questions at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b96WIvouLQ-7",
        "outputId": "0a20a144-bbc7-4455-ea3e-110cc12a6161"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If I am 6 ft 4 inches, how tall am I in centimeters\n"
          ]
        }
      ],
      "source": [
        "multi_template = \"\"\"Answer the following questions one at a time.\n",
        "\n",
        "Questions:\n",
        "{questions}\n",
        "\n",
        "Answers:\n",
        "\"\"\"\n",
        "long_prompt = PromptTemplate(\n",
        "    template=multi_template,\n",
        "    input_variables=[\"questions\"]\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=long_prompt,\n",
        "    llm=flan_t5\n",
        ")\n",
        "\n",
        "qs_str = (\n",
        "    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n",
        "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
        "    \"Who was the 12th person on the moon?\" +\n",
        "    \"How many eyes does a blade of grass have?\"\n",
        ")\n",
        "\n",
        "print(llm_chain.run(qs_str))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url_template = \"\"\"Answer the following questions one at a time.\n",
        "\n",
        "Questions:\n",
        "{questions}\n",
        "\n",
        "Answers:\n",
        "\"\"\"\n",
        "\n",
        "template = \"\"\"What is the enterprise name associated with this link {url}?\n",
        "\n",
        "Answer: \"\"\"\n",
        "const_prompt = PromptTemplate({\n",
        "  template: template,\n",
        "  inputVariables: [\"questions\",\"url\"],\n",
        "});"
      ],
      "metadata": {
        "id": "NFFcjlgmlzKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y99CMKSbOqBy"
      },
      "source": [
        "But with this model it doesn't work too well, we'll see this approach works better with different models soon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpdXG9YtzrLJ"
      },
      "source": [
        "## OpenAI\n",
        "\n",
        "Start by installing additional prerequisites:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHo2YRHPDgHH",
        "outputId": "72172782-a8bb-4e9b-a488-1fcf047b9ed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.3 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 KB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for openai (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fOo9qQvDgkz"
      },
      "source": [
        "We can also use OpenAI's generative models. The process is similar, we need to\n",
        "give our API key which can be retrieved by signing up for an account on the\n",
        "[OpenAI website](https://openai.com/api/) (see top-right of page). We then pass the API key below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "deWmOJecfbBr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "OPENAI_API_KEY = ''\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU4xirWX-Ds4"
      },
      "source": [
        "If using OpenAI via Azure you should also set:\n",
        "\n",
        "```python\n",
        "os.environ['OPENAI_API_TYPE'] = 'azure'\n",
        "# API version to use (Azure has several)\n",
        "os.environ['OPENAI_API_VERSION'] = '2022-12-01'\n",
        "# base URL for your Azure OpenAI resource\n",
        "os.environ['OPENAI_API_BASE'] = 'https://your-resource-name.openai.azure.com'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AWnaTCP0Ryg"
      },
      "source": [
        "Then we decide on which model we'd like to use, there are several options but we will go with `text-davinci-003`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ZhQSDoYe0ly4"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "davinci = OpenAI(model_name='text-davinci-003')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NvK4o6SDrs0"
      },
      "source": [
        "Alternatively if using Azure OpenAI we do:\n",
        "\n",
        "```python\n",
        "from langchain.llms import AzureOpenAI\n",
        "\n",
        "llm = AzureOpenAI(\n",
        "    deployment_name=\"your-azure-deployment\", \n",
        "    model_name=\"text-davinci-003\"\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGL2zs3uEVj6"
      },
      "source": [
        "We'll use the same simple question-answer prompt template as before with the Hugging Face example. The only change is that we now pass our OpenAI LLM `davinci`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVSsC3iGEPAp",
        "outputId": "786a6000-811e-4da5-a6d1-8c605c5d049b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The Green Bay Packers won the Super Bowl in the 2010 season.\n"
          ]
        }
      ],
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "question = \"Which NFL team won the Super Bowl in the 2010 season?\"\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=davinci\n",
        ")\n",
        "\n",
        "print(llm_chain.run(question))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL-buasOKpKs"
      },
      "source": [
        "The same works again for multiple questions using `generate`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMua1MWcKtSx",
        "outputId": "55efeae1-8c30-4069-a2a8-fb78212f8523"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text=' The Green Bay Packers won the Super Bowl in the 2010 season.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' 193.04 centimeters', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' Eugene A. Cernan was the 12th person to walk on the moon. He was part of the Apollo 17 mission in December 1972.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' A blade of grass does not have any eyes.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 131, 'prompt_tokens': 75, 'completion_tokens': 56}})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qs = [\n",
        "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
        "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
        "    {'question': \"Who was the 12th person on the moon?\"},\n",
        "    {'question': \"How many eyes does a blade of grass have?\"}\n",
        "]\n",
        "llm_chain.generate(qs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6LPiF-qiktm"
      },
      "source": [
        "Note that the below format doesn't feed the questions in iteratively but instead all in one chunk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2-es7SgFddS",
        "outputId": "d16b7afc-f0d1-4f6a-9d89-f6811839bb02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "1. The New Orleans Saints \n",
            "2. 193 centimeters \n",
            "3. Harrison Schmitt \n",
            "4. Zero.\n"
          ]
        }
      ],
      "source": [
        "qs = [\n",
        "    \"Which NFL team won the Super Bowl in the 2010 season?\",\n",
        "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\",\n",
        "    \"Who was the 12th person on the moon?\",\n",
        "    \"How many eyes does a blade of grass have?\"\n",
        "]\n",
        "print(llm_chain.run(qs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx6Ua9wNiktm"
      },
      "source": [
        "Now we can try to answer all question in one go, as mentioned, more powerful LLMs like `text-davinci-003` will be more likely to handle these more complex queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbjxnnVzA47s",
        "outputId": "c135b066-fbf0-4516-a22a-9ea3e6d5d5b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The New Orleans Saints won the Super Bowl in the 2010 season.\n",
            "If you are 6 ft 4 inches, you are 193.04 centimeters tall.\n",
            "The 12th person on the moon was Harrison H. Schmitt.\n",
            "A blade of grass does not have eyes.\n"
          ]
        }
      ],
      "source": [
        "multi_template = \"\"\"Answer the following questions one at a time.\n",
        "\n",
        "Questions:\n",
        "{questions}\n",
        "\n",
        "Answers:\n",
        "\"\"\"\n",
        "long_prompt = PromptTemplate(\n",
        "    template=multi_template,\n",
        "    input_variables=[\"questions\"]\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=long_prompt,\n",
        "    llm=davinci\n",
        ")\n",
        "\n",
        "qs_str = (\n",
        "    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n",
        "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
        "    \"Who was the 12th person on the moon?\" +\n",
        "    \"How many eyes does a blade of grass have?\"\n",
        ")\n",
        "\n",
        "print(llm_chain.run(qs_str))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multi_template = \"\"\"Answer the following questions one at a time.\n",
        "\n",
        "Questions:\n",
        "{questions}\n",
        "\n",
        "Answers:\n",
        "\"\"\"\n",
        "long_prompt = PromptTemplate(\n",
        "    template=multi_template,\n",
        "    input_variables=[\"questions\"]\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=long_prompt,\n",
        "    llm=davinci\n",
        ")\n",
        "\n",
        "qs_str = (\n",
        "    \"What is the enterprise associated with this web adrress: {url}?\\n\" +\n",
        "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
        "    \"Who was the 12th person on the moon?\" +\n",
        "    \"How many eyes does a blade of grass have?\"\n",
        ")\n",
        "\n",
        "print(llm_chain.run(qs_str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcr678uQ1X04",
        "outputId": "d0655a03-5afb-4cee-b64b-bfb646b821a4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The enterprise associated with this web address {url} cannot be determined without knowing the specific URL.\n",
            "\n",
            "6 ft 4 inches is 193.04 centimeters.\n",
            "\n",
            "The 12th person on the moon was Harrison Schmitt.\n",
            "\n",
            "A blade of grass has no eyes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template1 = \"\"\"Answer the following questions one at a time.\n",
        "\n",
        "        Questions:\n",
        "        What is the enterprise associated with this web address: {url}?\\n\\n\n",
        "        \"\"\"\n",
        "prompt_template1 = PromptTemplate(input_variables=[\"url\"], template=template1)\n",
        "ent_name_chain = LLMChain(llm=davinci, prompt=prompt_template1)\n",
        "### Chain2:\n",
        "template2 = \"\"\"What is the online revenue of this {enterprise} for fiscal year 2021?\n",
        "\\n\\n\n",
        "\"\"\"\n",
        "prompt_template2 = PromptTemplate(input_variables=[\"enterprise\"], template=template2)\n",
        "ent_rev_chain = LLMChain(llm=davinci, prompt=prompt_template2)\n",
        "\n",
        "ent_rev_chain_seq = SimpleSequentialChain(\n",
        "        chains=[ent_name_chain, ent_rev_chain], verbose=True\n",
        "    )\n",
        "\n",
        "\n",
        "print(ent_rev_chain_seq.run(\"www.gapinc.com\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMJHO5SNANT2",
        "outputId": "6007f4f1-39f9-4a8a-86c6-5fbab6a91049"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mAnswer: The enterprise associated with this web address is Gap Inc., an American multinational clothing and accessories retailer.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m\n",
            "\n",
            "Gap Inc. reported total net sales of $16.4 billion for fiscal year 2021. Online revenue for the same period was $4.7 billion, representing 28.7% of total net sales.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "Gap Inc. reported total net sales of $16.4 billion for fiscal year 2021. Online revenue for the same period was $4.7 billion, representing 28.7% of total net sales.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template1.format(url='www.gapinc.com')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 59
        },
        "id": "B6u8Fb73Glb5",
        "outputId": "58aa8d8e-a751-4457-c829-b4a173087605"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer the following questions one at a time.\\n\\n        Questions:\\n        What is the enterprise associated with this web address: www.gapinc.com?\\n\\n\\n        '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybMkI18xfbBr"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12 (main, Apr  5 2022, 01:52:34) \n[Clang 12.0.0 ]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}